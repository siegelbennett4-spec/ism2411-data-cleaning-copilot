# data_cleaning.py
# Purpose: Load a messy sales CSV, apply a clear sequence of cleaning steps,
# and write a cleaned CSV suitable for analysis.

import pandas as pd
import numpy as np
from typing import Tuple

# --- Copilot-assisted function ---
# What: Load a CSV file into a pandas DataFrame.
# Why: Centralize file loading to handle common parameters and encoding issues.
# (I triggered Copilot here for a suggested implementation and then adapted it.)
def load_data(file_path: str) -> pd.DataFrame:
    """Load CSV from `file_path` into a DataFrame.
    Returns a pandas DataFrame.
    """
    try:
        df = pd.read_csv(file_path)
    except Exception:
        # fallback: try with latin-1 encoding
        df = pd.read_csv(file_path, encoding="latin-1")
    return df

# --- Copilot-assisted function ---
# What: Standardize column names to lowercase with underscores.
# Why: Consistent column names make later selection and transformations reliable.
# (Generated by Copilot then simplified to our style.)
def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Standardize column names: strip, lower, replace spaces with underscores."""
    df = df.copy()
    df.columns = [
        str(col).strip().lower().replace(" ", "_").replace("-", "_")
        for col in df.columns
    ]
    return df

# What: Strip whitespace from string columns we care about (product_name, category).
# Why: Leading/trailing spaces cause duplicate categories/names and hamper grouping.
def strip_text_fields(df: pd.DataFrame, fields: list) -> pd.DataFrame:
    df = df.copy()
    for f in fields:
        if f in df.columns:
            df[f] = df[f].astype(str).str.strip()
            # turn empty strings back into NaN for consistent missing-value handling
            df.loc[df[f] == "", f] = np.nan
    return df

# --- Copilot-assisted function ---
# What: Handle missing numeric values for price and quantity.
# Why: Decide a consistent policy (fill or drop) so downstream analysis isn't broken.
# Copilot suggested a strategy; I adjusted it to our chosen policy below.
def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:
    """Fill or drop missing values for key numeric fields.

    Policy used here:
    - If `price` is missing or non-numeric -> drop the row (price is required).
    - If `quantity` is missing -> fill with the median quantity (rounded to int).
      Median imputation is more robust than filling with 1 when quantities vary.
    """
    df = df.copy()

    # coerce numeric fields; non-numeric become NaN
    if "price" in df.columns:
        df["price"] = pd.to_numeric(df["price"], errors="coerce")
    if "quantity" in df.columns:
        df["quantity"] = pd.to_numeric(df["quantity"], errors="coerce")

    # Drop rows without a valid price
    if "price" in df.columns:
        before = len(df)
        df = df[~df["price"].isna()]
        # optional: reset index
        df = df.reset_index(drop=True)

    # Fill missing quantities with the median of valid (non-negative) quantities.
    if "quantity" in df.columns:
        valid_q = df.loc[df["quantity"].notna() & (df["quantity"] >= 0), "quantity"]
        median_q = valid_q.median()
        if np.isnan(median_q):
            median_q = 1
        df["quantity"] = df["quantity"].fillna(median_q).astype(int)

    return df

# What: Remove rows with clearly invalid values (negative price/quantity)
# Why: Negative prices or quantities are data entry errors for this dataset.
def remove_invalid_rows(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "quantity" in df.columns:
        df = df[df["quantity"] >= 0]
    if "price" in df.columns:
        df = df[df["price"] >= 0]
    df = df.reset_index(drop=True)
    return df

# Additional helpful step: add a calculated `total` column
# What: Compute total = price * quantity
# Why: Useful for verification and downstream summaries
def add_total_column(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "price" in df.columns and "quantity" in df.columns:
        df["total"] = df["price"] * df["quantity"]
    return df

# Full pipeline function (not required but convenient for testing)
def run_cleaning_pipeline(raw_path: str, cleaned_path: str) -> pd.DataFrame:
    df = load_data(raw_path)
    df = clean_column_names(df)
    df = strip_text_fields(df, ["product_name", "category"])
    df = handle_missing_values(df)
    df = remove_invalid_rows(df)
    df = add_total_column(df)
    df.to_csv(cleaned_path, index=False)
    return df

if __name__ == "__main__":
    raw_path = "data/raw/sales_data_raw.csv"
    cleaned_path = "data/processed/sales_data_clean.csv"

    df_raw = load_data(raw_path)
    df_clean = clean_column_names(df_raw)
    df_clean = strip_text_fields(df_clean, ["product_name", "category"])
    df_clean = handle_missing_values(df_clean)
    df_clean = remove_invalid_rows(df_clean)
    df_clean = add_total_column(df_clean)
    df_clean.to_csv(cleaned_path, index=False)

    print("Cleaning complete. First few rows:")
    print(df_clean.head())
